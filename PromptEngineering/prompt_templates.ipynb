{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da4a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Templates --- Structured, reusable text templates used in LLM apps to standardize and format instructions\n",
    "#or queries you send to an AI model.\n",
    "\n",
    "#Types of Prompt Templates\n",
    "\n",
    "# 1. SystemMessagePromptTemplate --- This template is used for generating system messages that provide model context or persona.\n",
    "# 2. HumanMessagePromptTemplate ---- This template is used for geenrating human message (representing user input)\n",
    "# 3. AIMessagePromptTemplate ------- Template for generating AI message, representing response from the assistant\n",
    "# 4. PromptTemplate ---------------- Basic Template class for creating prompts with static text and variable placeholders\n",
    "# 5. ChatPromptTemplate ------------ Template for creating prompts with a sequence of message types in chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea41d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "#LLM\n",
    "#temperature decides the following\n",
    "# 0 ----- model will only emit facts\n",
    "# 1 ----- model will be super creative\n",
    "#\n",
    "#  0 --------------------------- 1\n",
    "# Factual                      Creative\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7624641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1. **Machine Learning and Deep Learning**: AI primarily relies on machine learning (ML) and deep learning (DL) techniques, which enable systems to learn from data and improve their performance over time. ML algorithms identify patterns in data, while DL uses neural networks to process complex data structures, such as images and natural language.\\n\\n2. **Applications Across Industries**: AI is transforming various sectors, including healthcare (diagnosis and treatment recommendations), finance (fraud detection and algorithmic trading), transportation (autonomous vehicles), and customer service (chatbots and virtual assistants). Its versatility allows for enhanced efficiency, accuracy, and decision-making.\\n\\n3. **Ethical Considerations and Challenges**: The rapid advancement of AI raises important ethical issues, such as bias in algorithms, data privacy, and the potential for job displacement. Addressing these challenges is crucial for ensuring that AI technologies are developed and deployed responsibly, promoting fairness and transparency.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 32, 'total_tokens': 219, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-C3WRedySaFxhOnt7b7UqeyJa79s3o', 'finish_reason': 'stop', 'logprobs': None}, id='run-d21e56d2-a2c5-4b77-9813-6b019a33cedd-0', usage_metadata={'input_tokens': 32, 'output_tokens': 187, 'total_tokens': 219, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = HumanMessage('Tell me about AI in 3 points')\n",
    "system = SystemMessage(\"Act as an AI expert. Your answers will be on-point.\")\n",
    "\n",
    "messages = [system,question]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "666b7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate,HumanMessagePromptTemplate,PromptTemplate,ChatMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db78fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('Act as an {persona} professor. You answer in short sentences')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('Tell me about the {topics} in {points} points')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a889fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['persona'], input_types={}, partial_variables={}, template='Act as an {persona} professor. You answer in short sentences'), additional_kwargs={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd0ed3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['points', 'topics'], input_types={}, partial_variables={}, template='Tell me about the {topics} in {points} points'), additional_kwargs={})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982d2258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='Act as an AI professor. You answer in short sentences', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system.format(persona='AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b593ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='Tell me about the Langchain in 3 points', additional_kwargs={}, response_metadata={})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.format(topics='Langchain', points=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d870a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [system,question]\n",
    "\n",
    "template = ChatPromptTemplate(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b2602c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Act as an gemologist professor. You answer in short sentences', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me about the Types of Stones in 2 points', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = template.invoke({'persona':'gemologist', 'topics':'Types of Stones','points':2})\n",
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8861a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d9c007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Precious Stones**: These are rare and valuable, including diamonds, rubies, sapphires, and emeralds. They are prized for their beauty and durability.\n",
      "\n",
      "2. **Semi-Precious Stones**: These are more abundant and include stones like amethyst, garnet, and turquoise. They are often used in jewelry but are generally less expensive than precious stones.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01280f5e",
   "metadata": {},
   "source": [
    "# New Version of LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952e3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "You are a helpful assistant that translates the {input_language} to {output_language}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45bb6e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "promptTemplate = PromptTemplate.from_template(\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "385ef404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant that translates the english to hindi\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptTemplate.format(input_language = \"english\", output_language=\"hindi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "536fcc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are a helpful assistant that translates the english to hindi\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "promptTemplate = PromptTemplate(\n",
    "    template=TEMPLATE ,\n",
    "    input_variables=[\"input_language\",\"output_language\"]\n",
    ")\n",
    "\n",
    "promptTemplate.format(input_language = \"english\", output_language=\"hindi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ed414",
   "metadata": {},
   "source": [
    "# Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c6794d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79beafa6",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92f8dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Interprete the text and evaluate the text.\n",
    "sentiment: is the text in a positive, neutral or negative sentiment?\n",
    "subject: What subject is the text about? Use exactly one word.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "sentiment\n",
    "subject\n",
    "\n",
    "text: {input}\n",
    "\n",
    "Examples:\n",
    "text: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\n",
    "sentiment: positive\n",
    "subject: BellaVista\n",
    "\n",
    "text: BellaVista restaurant was alright. The food was decent, but nothing stood out.\n",
    "sentiment: neutral\n",
    "subject: BellaVista\n",
    "\n",
    "text: I was disappointed with BellaVista. The service was slow and the dishes lacked flavor.\n",
    "sentiment: negative\n",
    "subject: BellaVista\n",
    "\n",
    "text: SeoulSavor offered the most authentic Korean flavors I've tasted outside of Seoul. The kimchi was perfectly fermented and spicy.\n",
    "sentiment: positive\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: SeoulSavor was okay. The bibimbap was good but the bulgogi was a bit too sweet for my taste.\n",
    "sentiment: neutral\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: I didn't enjoy my meal at SeoulSavor. The tteokbokki was too mushy and the service was not attentive.\n",
    "sentiment: negative\n",
    "subject: SeoulSavor\n",
    "\n",
    "text: MunichMeals has the best bratwurst and sauerkraut I've tasted outside of Bavaria. Their beer garden ambiance is truly authentic.\n",
    "sentiment: positive\n",
    "subject: MunichMeals\n",
    "\n",
    "text: MunichMeals was alright. The weisswurst was okay, but I've had better elsewhere.\n",
    "sentiment: neutral\n",
    "subject: MunichMeals\n",
    "\n",
    "text: I was let down by MunichMeals. The potato salad lacked flavor and the staff seemed uninterested.\n",
    "sentiment: negative\n",
    "subject: MunichMeals\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c479b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"text\": \"The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\",\n",
    "        \"response\": \"sentiment: positive\\nsubject: BellaVista\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"BellaVista restaurant was alright. The food was decent, but nothing stood out.\",\n",
    "        \"response\": \"sentiment: neutral\\nsubject: BellaVista\"\n",
    "    },\n",
    "    ### other examples are left out\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "exampleQuestion = PromptTemplate(input_variables=[\"text\",\"response\"], template=\"text: {text}\\n{response}\")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=exampleQuestion,\n",
    "    suffix=\"text: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "293b7c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: The BellaVista restaurant offers an exquisite dining experience. The flavors are rich and the presentation is impeccable.\n",
      "sentiment: positive\n",
      "subject: BellaVista\n",
      "\n",
      "text: BellaVista restaurant was alright. The food was decent, but nothing stood out.\n",
      "sentiment: neutral\n",
      "subject: BellaVista\n",
      "\n",
      "text: My experience here was bad\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(input=\"My experience here was bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66333571",
   "metadata": {},
   "source": [
    "# Chain of Thoughts Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Introduction\n",
    "introduction_template = \"\"\"\n",
    "Interprete the text and evaluate the text. Determine if the text has a positive, neutral, or negative sentiment. \n",
    "Also, identify the subject of the text in one word.\n",
    "\"\"\"\n",
    "introduction_prompt = PromptTemplate.from_template(introduction_template)\n",
    "\n",
    "# Example\n",
    "example_template = \"\"\"\n",
    "Chain-of-Thought Prompts:\n",
    "Let's start by evaluating a statement. Consider: \"{example_text}\". How does this make you feel about {example_subject}?\n",
    "Response: {example_evaluation}\n",
    "\n",
    "Based on the {example_sentiment} nature of that statement, how would you format your response?\n",
    "Response: {example_format}\n",
    "\"\"\"\n",
    "example_prompt = PromptTemplate.from_template(example_template)\n",
    "\n",
    "# Execution\n",
    "execution_template = \"\"\"\n",
    "Now, execute this process for the text: \"{input}\".\n",
    "\"\"\"\n",
    "execution_prompt = PromptTemplate.from_template(execution_template)\n",
    "\n",
    "\n",
    "\n",
    "# Composing the full prompt\n",
    "full_template = \"\"\"{introduction}\n",
    "\n",
    "{example}\n",
    "\n",
    "{execution}\"\"\"\n",
    "full_prompt = PromptTemplate.from_template(full_template)\n",
    "\n",
    "\n",
    "# PipelinePrompts\n",
    "input_prompts = [\n",
    "    (\"introduction\", introduction_prompt),\n",
    "    (\"example\", example_prompt),\n",
    "    (\"execution\", execution_prompt)\n",
    "]\n",
    "\n",
    "\n",
    "pipeline_prompt = PipelinePromptTemplate(final_prompt=full_prompt, pipeline_prompts=input_prompts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a7a91",
   "metadata": {},
   "source": [
    "How to use this chain of thoughts prompt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c716696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statement expresses a negative impression because it criticizes the advisors for being slow and unprofessional. \n",
      "\n",
      "Based on the negative nature of that statement, my response would be: The text has a negative sentiment. The subject is advisors.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Instantiate the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the LangChain Expression Language (LCEL) chain\n",
    "chain = pipeline_prompt | llm | output_parser\n",
    "\n",
    "\n",
    "# Define the input variables\n",
    "input_data = {\n",
    "    \"example_text\": \"The new product launch was a massive success, exceeding all sales expectations.\",\n",
    "    \"example_subject\": \"launch\",\n",
    "    \"example_evaluation\": \"The statement gives a positive impression because it talks about success and exceeding expectations.\",\n",
    "    \"example_sentiment\": \"positive\",\n",
    "    \"example_format\": \"The text has a positive sentiment. The subject is launch.\",\n",
    "    \"input\": \"The advisors at Eadvice are very slow and unprofessional.\"\n",
    "}\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke(input_data)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f22ec4",
   "metadata": {},
   "source": [
    "The statement expresses a negative impression because it criticizes the advisors for being slow and unprofessional. \n",
    "\n",
    "Based on the negative nature of that statement, my response would be: The text has a negative sentiment. The subject is advisors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecf7df",
   "metadata": {},
   "source": [
    "# Serializing Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"input\"], template=\"Tell me a joke about {input}\")\n",
    "\n",
    "prompt.save(\"prompt.json\")\n",
    "## or in yaml format: prompt.save(\"prompt.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed08d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me a joke about AI'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "promptNew = load_prompt(\"prompt.json\")\n",
    "\n",
    "promptNew.format(input=\"AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc2d9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
