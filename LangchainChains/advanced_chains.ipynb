{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/5_fr35dx4bg267_nq27dz3tw0000gn/T/ipykernel_33059/1191947449.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt_template)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'a parrot',\n",
       " 'text': 'Why did the parrot get a timeout?\\n\\nBecause it kept telling the same old \"polly\" jokes!'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.llm import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
    "\n",
    "prompt_template = PromptTemplate(input_variables=[\"input\"], template=\"Tell me a joke about {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.invoke(input=\"a parrot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'a parrot',\n",
       " 'language': 'hindi',\n",
       " 'text': '‡§è‡§ï ‡§Ü‡§¶‡§Æ‡•Ä ‡§®‡•á ‡§è‡§ï ‡§§‡•ã‡§§‡•á ‡§ï‡•ã ‡§ñ‡§∞‡•Ä‡§¶‡§æ ‡§î‡§∞ ‡§¨ seller ‡§∏‡•á ‡§™‡•Ç‡§õ‡§æ, \"‡§Ø‡•á ‡§§‡•ã‡§§‡§æ ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•à ‡§ï‡•ç‡§Ø‡§æ?\"\\n\\n‡§¨ seller ‡§¨‡•ã‡§≤‡§æ, \"‡§π‡§æ‡§Å, ‡§¨‡§ø‡§≤‡•ç‡§ï‡•Å‡§≤! ‡§Ö‡§ó‡§∞ ‡§Ü‡§™ ‡§á‡§∏‡•á ‡§∞‡•ã‡§ú‡§º ‡§∏‡§π‡•Ä ‡§∏‡•á ‡§¨‡§æ‡§§‡•á‡§Ç ‡§ï‡§∞‡•á‡§Ç‡§ó‡•á ‡§§‡•ã ‡§Ø‡•á ‡§ï‡•Å‡§õ ‡§≠‡•Ä ‡§¨‡•ã‡§≤‡§®‡•á ‡§≤‡§ó‡•á‡§ó‡§æ‡•§\"\\n\\n‡§Ü‡§¶‡§Æ‡•Ä ‡§®‡•á ‡§ï‡§π‡§æ, \"‡§†‡•Ä‡§ï ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§á‡§∏‡•á ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§∏‡§ø‡§ñ‡§æ‡§ä‡§Ç‡§ó‡§æ‡•§\"\\n\\n‡§ï‡•Å‡§õ ‡§¶‡§ø‡§®‡•ã‡§Ç ‡§¨‡§æ‡§¶ ‡§§‡•ã‡§§‡•á ‡§®‡•á ‡§¨‡•ã‡§≤‡§®‡§æ ‡§∂‡•Å‡§∞‡•Ç ‡§ï‡§ø‡§Ø‡§æ, \"‡§∏‡•Å‡§™‡§∞‡§∏‡•ç‡§ü‡§æ‡§∞, ‡§∏‡•Å‡§™‡§∞‡§∏‡•ç‡§ü‡§æ‡§∞!\"\\n\\n‡§Ü‡§¶‡§Æ‡•Ä ‡§π‡•à‡§∞‡§æ‡§® ‡§π‡•Å‡§Ü ‡§î‡§∞ ‡§¨‡•ã‡§≤‡§æ, \"‡§Ø‡•á ‡§§‡•ã ‡§ï‡•à‡§∏‡§æ ‡§¨‡•ã‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à?\"\\n\\n‡§§‡•ã‡§§‡§æ ‡§¨‡•ã‡§≤‡§æ, \"‡§ï‡•ç‡§Ø‡•ã‡§Ç‡§ï‡§ø ‡§§‡•Å‡§Æ ‡§π‡§Æ‡•á‡§∂‡§æ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§´‡§ø‡§≤‡•ç‡§Æ‡•Ä ‡§°‡§æ‡§Ø‡§≤‡•â‡§ó‡•ç‡§∏ ‡§π‡•Ä ‡§§‡•ã ‡§¶‡•ã‡§π‡§∞‡§æ‡§§‡•á ‡§π‡•ã!\" üòÑ'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables=[\"input\", \"language\"], template=\"Tell me a joke about {input} in {language}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "chain.invoke({\"input\": \"a parrot\", \"language\": \"hindi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be more complex and not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.sequential import SequentialChain\n",
    "\n",
    "# This is an LLMChain to write a review given a dish name and the experience.\n",
    "prompt_review = PromptTemplate.from_template(\n",
    "    template=\"You ordered {dish_name} from {restaurantName} and your experience was {experience}. Write a review: \"\n",
    ")\n",
    "chain_review = LLMChain(llm=llm, prompt=prompt_review, output_key=\"review\")\n",
    "\n",
    "#=============================================================================================================\n",
    "\n",
    "# This is an LLMChain to write a follow-up comment given the restaurant review.\n",
    "prompt_comment = PromptTemplate.from_template(\n",
    "    template=\"Given the restaurant review: {review}, write a follow-up comment: \"\n",
    ")\n",
    "chain_comment = LLMChain(llm=llm, prompt=prompt_comment, output_key=\"comment\")\n",
    "\n",
    "#==============================================================================================================\n",
    "\n",
    "# This is an LLMChain to summarize a review.\n",
    "prompt_summary = PromptTemplate.from_template(\n",
    "    template=\"Summarise the review in one short sentence: \\n\\n {comment}\"\n",
    ")\n",
    "chain_summary = LLMChain(llm=llm, prompt=prompt_summary, output_key=\"summary\")\n",
    "\n",
    "#================================================================================================================\n",
    "\n",
    "# This is an LLMChain to translate a summary into German.\n",
    "prompt_translation = PromptTemplate.from_template(\n",
    "    template=\"\"\"    \n",
    "    Translate the summary to 3 languages: \n",
    "    1. Hindi\n",
    "    2. Spanish\n",
    "    3. French\n",
    "    \\n\\n {summary}\n",
    "    \"\"\"\n",
    ")\n",
    "chain_translation = LLMChain(\n",
    "    llm=llm, prompt=prompt_translation, output_key=\"lang_translation\"\n",
    ")\n",
    "\n",
    "#=================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dish_name': 'Chicken Biryani',\n",
       " 'restaurantName': 'Akbar Biryani',\n",
       " 'experience': 'It was awful!',\n",
       " 'review': \"**Review of Akbar Biryani**\\n\\n‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ\\n\\nI recently ordered Chicken Biryani from Akbar Biryani, and I must say my experience was truly disappointing. \\n\\nFirst, let's talk about the flavor. Biryani is supposed to be a symphony of spices, but what I received was bland and unremarkable. The chicken was overcooked, making it dry and chewy, and the rice was far from the fluffy, aromatic layers that one expects in a good biryani. \\n\\nPresentation-wise, the dish looked dull and unappetizing. There was no vibrant color nor any garnish to enhance its visual appeal. Instead of feeling excited to dive into my meal, I found myself unenthused and somewhat regretful of my choice.\\n\\nAdditionally, the portion size was underwhelming. For the price I paid, I expected a hearty serving, but it felt more like a snack than a fulfilling meal. It left me unsatisfied, and I found myself needing something else shortly after.\\n\\nTo top it off, the delivery took longer than anticipated, which only added to my frustration. When it finally arrived, it was lukewarm, further detracting from the experience.\\n\\nOverall, I had high hopes for Akbar Biryani, but they missed the mark completely. I wouldn't recommend this place if you're looking for an authentic and enjoyable biryani experience. There are many other spots in town that do it far better. I definitely won't be returning.\",\n",
       " 'comment': \"I'm really sorry to hear about your experience at Akbar Biryani. It‚Äôs always disappointing when a restaurant doesn‚Äôt meet expectations, especially when you're craving something as flavorful as biryani. It‚Äôs important for food to be both delicious and satisfying, so I can understand your frustration with the overcooked chicken and bland flavors. \\n\\nHave you had an opportunity to try any other biryani places that you enjoyed? It might be worth exploring other options in town to find that authentic taste you were looking forward to. Thank you for sharing your review; it helps others make informed choices!\",\n",
       " 'summary': \"The review expresses disappointment with Akbar Biryani's overcooked chicken and bland flavors, suggesting the exploration of other biryani restaurants for a better experience.\",\n",
       " 'hindi_translation': '‡§∏‡§Æ‡•Ä‡§ï‡•ç‡§∑‡§æ ‡§Æ‡•á‡§Ç ‡§Ö‡§ï‡§¨‡§∞ ‡§¨‡§ø‡§∞‡§Ø‡§æ‡§®‡•Ä ‡§ï‡•á ‡§Ö‡§ß‡§ø‡§ï ‡§™‡§ï‡•á ‡§π‡•Å‡§è ‡§ö‡§ø‡§ï‡§® ‡§î‡§∞ ‡§´‡•Ä‡§ï‡•Ä ‡§∏‡•ç‡§µ‡§æ‡§¶‡•ã‡§Ç ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø ‡§®‡§ø‡§∞‡§æ‡§∂‡§æ ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§ ‡§ï‡•Ä ‡§ó‡§à ‡§π‡•à, ‡§î‡§∞ ‡§¨‡•á‡§π‡§§‡§∞ ‡§Ö‡§®‡•Å‡§≠‡§µ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ö‡§®‡•ç‡§Ø ‡§¨‡§ø‡§∞‡§Ø‡§æ‡§®‡•Ä ‡§∞‡•á‡§∏‡•ç‡§§‡§∞‡§æ‡§Ç ‡§ï‡§æ ‡§™‡§§‡§æ ‡§≤‡§ó‡§æ‡§®‡•á ‡§ï‡§æ ‡§∏‡•Å‡§ù‡§æ‡§µ ‡§¶‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ ‡§π‡•à‡•§'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_review, chain_comment, chain_summary, chain_translation],\n",
    "    input_variables=[\"dish_name\",\"restaurantName\",\"experience\"],\n",
    "    output_variables=[\"review\", \"comment\", \"summary\", \"lang_translation\"],\n",
    ")\n",
    "\n",
    "overall_chain.invoke({\"dish_name\": \"Chicken Biryani\", \"restaurantName\":\"Akbar Biryani\", \"experience\": \"It was awful!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of chaining multiple chains together we can also use an LLM to decide which follow up chain is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "\n",
    "positive_template = \"\"\"You are an AI that focuses on the positive side of things. \\\n",
    "Whenever you analyze a text, you look for the positive aspects and highlight them. \\\n",
    "Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \\\n",
    "not favoring any positive or negative aspects. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \\\n",
    "You analyze a text and show the potential downsides. Here is the text:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"positive\",\n",
    "        \"description\": \"Good for analyzing positive sentiments\",\n",
    "        \"prompt_template\": positive_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neutral\",\n",
    "        \"description\": \"Good for analyzing neutral sentiments\",\n",
    "        \"prompt_template\": neutral_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"negative\",\n",
    "        \"description\": \"Good for analyzing negative sentiments\",\n",
    "        \"prompt_template\": negative_template,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive': LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an AI that focuses on the positive side of things. Whenever you analyze a text, you look for the positive aspects and highlight them. Here is the text:\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x119ec9b90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11a29d350>, root_client=<openai.OpenAI object at 0x119d1e410>, root_async_client=<openai.AsyncOpenAI object at 0x11a2821d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'neutral': LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, not favoring any positive or negative aspects. Here is the text:\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x119ec9b90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11a29d350>, root_client=<openai.OpenAI object at 0x119d1e410>, root_async_client=<openai.AsyncOpenAI object at 0x11a2821d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}),\n",
       " 'negative': LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an AI that is designed to find the negative aspects in a text. You analyze a text and show the potential downsides. Here is the text:\\n{input}'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x119ec9b90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11a29d350>, root_client=<openai.OpenAI object at 0x119d1e410>, root_async_client=<openai.AsyncOpenAI object at 0x11a2821d0>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={})}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# prompt_infos is expected to be a list of dictionaries, each containing at least \"name\" and \"prompt_template\" keys.\n",
    "# For each prompt info:\n",
    "#   Extract the name and prompt template.\n",
    "#   Create a PromptTemplate object that takes one input variable called \"input\".\n",
    "#   Create an LLMChain: This is an LLM + a prompt. So, each \"destination\" (e.g., \"food_review\", \"hotel_review\", etc.) gets its own LLMChain.\n",
    "#   Store each chain in the destination_chains dictionary, keyed by its name.\n",
    "\n",
    "# Result: A dictionary where each key is a destination name and each value is a ready-to-use LLMChain for that intent.\n",
    "    \n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "destination_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: Good for analyzing positive sentiments\n",
      "neutral: Good for analyzing neutral sentiments\n",
      "negative: Good for analyzing negative sentiments\n"
     ]
    }
   ],
   "source": [
    "# For each prompt in prompt_infos, create a string like \"food_review: Handles food related reviews\".\n",
    "# Combine all such strings into a newline-separated list (destinations_str).\n",
    "# Print it for debugging. This will help the router know what all \"destinations\" it can send queries to.\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "positive: {'input': 'I ordered Pizza Salami for 9.99$ and it was awesome!'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'I ordered Pizza Salami for 9.99$ and it was awesome!',\n",
       " 'text': \"That's fantastic! It sounds like you had a wonderful experience with your Pizza Salami. The price of $9.99 is quite reasonable, and it's great to hear that the pizza was awesome! Enjoying a delicious meal is always a positive highlight of the day. üçï\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a predefined template (MULTI_PROMPT_ROUTER_TEMPLATE) that expects the available destinations.\n",
    "# Plug in the list of destinations you just built.\n",
    "# Build a PromptTemplate for the router, which will use the user input and output a routing decision.\n",
    "# Set the output_parser to RouterOutputParser() (this parses the router‚Äôs decision).\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "\n",
    "# Create a Router Chain that will use your LLM and the router prompt.\n",
    "# When given an input, it decides which destination chain (from step 1) to route the input to.\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "\n",
    "# MultiPromptChain is the master orchestrator.\n",
    "# It uses:\n",
    "# The router chain to select a destination.\n",
    "# The dictionary of destination chains.\n",
    "# A default chain (for \"neutral\" or unknown inputs).\n",
    "# verbose=True gives you logs as it routes and processes.\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=destination_chains[\"neutral\"],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\": \"I ordered Pizza Salami for 9.99$ and it was awesome!\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
