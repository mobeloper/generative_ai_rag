{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74627dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "933cd23e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "üß† Why Do We Need Chunking?\n",
    "\n",
    "1. LLMs have a context limit\n",
    "GPT-4, Claude, etc., have a max token window (e.g., 8k, 32k, 128k tokens).\n",
    "If your document is too long, it cannot fit in the prompt.\n",
    "‚úÖ Chunking breaks it into digestible parts.\n",
    "\n",
    "2. Improves retrieval accuracy\n",
    "Vector stores compare query embeddings with chunk embeddings.\n",
    "Long documents have lots of irrelevant info = bad match.\n",
    "‚úÖ Chunking ensures that only the most relevant parts are retrieved.\n",
    "\n",
    "3. Faster search and lower cost\n",
    "Embedding a full document is expensive and semantically noisy.\n",
    "Smaller chunks are faster to embed and compare.\n",
    "‚úÖ Efficient + cheaper.\n",
    "\n",
    "4. Prevents dilution of meaning\n",
    "If you embed an entire book, the \"vector\" represents everything = nothing specific.\n",
    "‚úÖ Chunking keeps meaning sharp and searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097be697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "abcde\n",
      "------------------------------\n",
      "Chunk 2:\n",
      "defgh\n",
      "------------------------------\n",
      "Chunk 3:\n",
      "ghijk\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Why?\n",
    "\n",
    "# Most LLMs perform best with short, focused ‚Äúchunks‚Äù of text.\n",
    "# Splitting documents ensures that your retrieval step pulls relevant, concise pieces rather than entire docs.\n",
    "\n",
    "# Why Chunking/Splitting?\n",
    "\n",
    "# LLMs have limited context windows (e.g., 4k‚Äì128k tokens).\n",
    "# Retrieval is more accurate with smaller, focused pieces (vs. giant docs).\n",
    "# Avoids irrelevant or noisy context for the LLM.\n",
    "\n",
    "# Goal:\n",
    "# Convert loaded documents into manageable, relevant text ‚Äúchunks‚Äù for embedding and retrieval.\n",
    "\n",
    "# 1. Load data\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"demo_document.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Chunk data\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3. Visualize\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n{'-'*30}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b39a164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'demo.txt'}, page_content='abcde'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='defgh'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='ghijk'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='jkl'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='This'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='is'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='an'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='exam'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='ample'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='of'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='usin'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='ing'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='text'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='file'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='in'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='lang'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='ngcha'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='hain'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='lang'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='ngcha'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='hain'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='is a'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='powe'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='werfu'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='ful'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='fram'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='amewo'),\n",
       " Document(metadata={'source': 'demo.txt'}, page_content='work')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks\n",
    "\n",
    "#This is an example of using text file in langchain\n",
    "#langchain is a powerful framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb4e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
