{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c2bf8e",
   "metadata": {},
   "source": [
    "## LangChain Expression Language (LCEL) Basics\n",
    "\n",
    "-  LangChain Expression Language is that any two runnables can be \"chained\" together into sequences. \n",
    "- The output of the previous runnable's .invoke() call is passed as input to the next runnable.\n",
    "- This can be done using the pipe operator (|), or the more explicit .pipe() method, which does the same thing.\n",
    "\n",
    "- Type of LCEL Chains\n",
    "    - SequentialChain\n",
    "    - Parallel Chain\n",
    "    - Router Chain\n",
    "    - Chain Runnables\n",
    "    - Custom Chain (Runnable Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb738f",
   "metadata": {},
   "source": [
    "1. What is LCEL?\n",
    "\n",
    "LCEL stands for LangChain Expression Language.\n",
    "It provides a concise, composable, and Pythonic way to build chains, pipelines, and workflows in LangChain.\n",
    "\n",
    "Goal: Replace the older SequentialChain, RouterChain, etc., with a more flexible and declarative API.\n",
    "Key Concepts: Chains as functions (or callables), easy composition (| operator), clear input/output typing.\n",
    "\n",
    "2. Core LCEL Components\n",
    "\n",
    "a. Runnable\n",
    "The core abstraction in LCEL.\n",
    "Any object that implements the .invoke() method and can be composed using the | operator.\n",
    "Examples: Models, PromptTemplates, Chains.\n",
    "\n",
    "b. PromptTemplate\n",
    "Used to format inputs into prompts for LLMs or ChatModels.\n",
    "LCEL expects prompt templates to have variable names matching the input keys.\n",
    "\n",
    "c. LLM and ChatModel\n",
    "Language Models and ChatModels from langchain_openai, langchain_community, etc.\n",
    "Compatible with LCEL if they implement .invoke()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caaf3727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9f5081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1183d38d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11839d250>, root_client=<openai.OpenAI object at 0x1185601d0>, root_async_client=<openai.AsyncOpenAI object at 0x11925c550>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import (\n",
    "                                        SystemMessagePromptTemplate,\n",
    "                                        HumanMessagePromptTemplate,\n",
    "                                        ChatPromptTemplate\n",
    "                                        )\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea34732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the sun, eight planets, and various other celestial bodies.\n",
      "2. The eight planets in order from the sun are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n",
      "3. The sun is a star at the center of the solar system, providing light and heat to the planets.\n",
      "4. The planets orbit the sun in elliptical paths, with each planet having its own unique characteristics.\n",
      "5. The solar system also includes moons, asteroids, comets, and dwarf planets like Pluto.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "question = template.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "\n",
    "response = llm.invoke(question)\n",
    "print(response.content)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af29dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "\n",
    "chain = template | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5d25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the sun, eight planets, and various other celestial bodies.\n",
      "2. The eight planets in order from the sun are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n",
      "3. The sun is a star at the center of the solar system, providing light and heat to the planets.\n",
      "4. The planets orbit the sun in elliptical paths, with each planet having its own unique characteristics.\n",
      "5. The solar system also includes moons, asteroids, comets, and dwarf planets like Pluto.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf78c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the Sun, eight planets, moons, asteroids, comets, and other celestial objects.\n",
      "2. The Sun is at the center of the solar system, with planets orbiting around it.\n",
      "3. The four inner planets are Mercury, Venus, Earth, and Mars, while the four outer planets are Jupiter, Saturn, Uranus, and Neptune.\n",
      "4. The asteroid belt lies between Mars and Jupiter, and comets are icy bodies that orbit the Sun.\n",
      "5. The solar system formed about 4.6 billion years ago from a giant cloud of gas and dust.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({'school': 'phd', 'topics': 'solar system', 'points': 5})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22ed93c",
   "metadata": {},
   "source": [
    "# What is StrOutputParser?\n",
    "\n",
    "\n",
    "StrOutputParser is a utility class in LangChain for handling the output of language models (LLMs/ChatModels).\n",
    "It parses the model’s response and returns it as a plain Python string, removing any unnecessary metadata, wrappers, or objects that the model or API may return.\n",
    "\n",
    "\n",
    "## Why do we need it?\n",
    "\n",
    "- By default, LLMs or chat models in LangChain might return structured objects, message objects, or dictionaries.\n",
    "- In most cases, especially for simple chains, you just want the generated text string (e.g., the answer, summary, completion, etc.).\n",
    "- StrOutputParser extracts this text from the raw output so your chain returns a clean string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e95d027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without StrParser\n",
      "\n",
      "content='The capital of India is New Delhi.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': None, 'id': 'chatcmpl-BvIPmqyz8mGlRObRn0tI2tYEE6Pz4', 'finish_reason': 'stop', 'logprobs': None} id='run-ecade9f1-3bf5-4621-8c53-5dfaee04de18-0' usage_metadata={'input_tokens': 19, 'output_tokens': 8, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "With StrParser\n",
      "\n",
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain1 = prompt | llm \n",
    "chain2 = prompt | llm | parser\n",
    "\n",
    "print(\"Without StrParser\\n\")\n",
    "resultChain1 = chain1.invoke({\"country\" , \"India\"})\n",
    "print(resultChain1)\n",
    "\n",
    "print(\"\\nWith StrParser\\n\")\n",
    "resultChain2 = chain2.invoke({\"country\": \"India\"})\n",
    "print(resultChain2)  # Output: \"The capital of India is New Delhi.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16fdc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the sun, eight planets, and various other celestial bodies.\n",
      "2. The eight planets in order from the sun are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.\n",
      "3. The sun is a star at the center of the solar system, providing light and heat to the planets.\n",
      "4. The planets orbit the sun in elliptical paths, with each planet having its own unique characteristics.\n",
      "5. The solar system also includes moons, asteroids, comets, and dwarf planets like Pluto.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "response = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 5})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db13a1",
   "metadata": {},
   "source": [
    "### Chaining Runnables (Chain Multiple Runnables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1b07c",
   "metadata": {},
   "source": [
    "- We can even combine this chain with more runnables to create another chain.\n",
    "- Let's see how easy our generated output is?\n",
    "\n",
    "\n",
    "# Concept: What Are Runnables and Chaining?\n",
    "\n",
    "Runnables in LangChain are modular, composable components—like prompts, LLMs, output parsers, or even custom functions—that implement an .invoke() method.\n",
    "\n",
    "Chaining Runnables means connecting these modular components in sequence, so the output of one becomes the input of the next. In LCEL, you use the pipe operator (|) to compose them functionally, much like Unix pipes or Pandas method chaining.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69b8368f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['points', 'school', 'topics'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['school'], input_types={}, partial_variables={}, template='You are {school} teacher. You answer in short sentences.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['points', 'topics'], input_types={}, partial_variables={}, template='tell me about the {topics} in {points} points'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1183d38d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11839d250>, root_client=<openai.OpenAI object at 0x1185601d0>, root_async_client=<openai.AsyncOpenAI object at 0x11925c550>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecbf745a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is relatively easy to understand as it provides basic information about the solar system and its components.\n"
     ]
    }
   ],
   "source": [
    "analysis_prompt = ChatPromptTemplate.from_template('''analyze the following text: {response}\n",
    "                                                   You need tell me that how difficult it is to understand.\n",
    "                                                   Answer in one sentence only.\n",
    "                                                   ''')\n",
    "\n",
    "fact_check_chain = analysis_prompt | llm | StrOutputParser()\n",
    "output = fact_check_chain.invoke({'response': response})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d028739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is relatively easy to understand as it provides basic information about the solar system and its components.\n"
     ]
    }
   ],
   "source": [
    "composed_chain = {\"response\": chain} | analysis_prompt | llm | StrOutputParser()\n",
    "\n",
    "output = composed_chain.invoke({'school': 'phd', 'topics': 'solar system', 'points': 5})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b4800d",
   "metadata": {},
   "source": [
    "### Parallel LCEL Chain\n",
    "- Parallel chains are used to run multiple runnables in parallel.\n",
    "- The final return value is a dict with the results of each value under its appropriate key.\n",
    "\n",
    "\n",
    "### What is a Parallel Chain?\n",
    "\n",
    "Parallel chaining lets you run multiple chains (pipelines) simultaneously using the same or similar inputs, collecting all outputs together.\n",
    "\n",
    "In LangChain LCEL, this is achieved via RunnableParallel.\n",
    "\n",
    "It’s analogous to a “fan-out” pattern, where you branch out your input to multiple tasks and gather all their results in one go.\n",
    "\n",
    "### Why Use Parallel Chains?\n",
    "\n",
    "- To answer multiple questions about the same input in one shot.\n",
    "- To generate diverse content (e.g., facts, poems, summaries) from the same base information.\n",
    "- To save code and avoid running chains sequentially when there’s no dependency between them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7bd2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the sun, eight planets, moons, asteroids, comets, and other celestial objects.\n",
      "2. The planets in the solar system orbit around the sun in elliptical paths.\n"
     ]
    }
   ],
   "source": [
    "system = SystemMessagePromptTemplate.from_template('You are {school} teacher. You answer in short sentences.')\n",
    "\n",
    "question = HumanMessagePromptTemplate.from_template('tell me about the {topics} in {points} points')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "fact_chain = template | llm | StrOutputParser()\n",
    "\n",
    "output = fact_chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ab5616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planets dance in space,\n",
      "Sun's light gives them grace.\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessagePromptTemplate.from_template('write a poem on {topics} in {sentences} lines')\n",
    "\n",
    "\n",
    "messages = [system, question]\n",
    "template = ChatPromptTemplate(messages)\n",
    "poem_chain = template | llm | StrOutputParser()\n",
    "\n",
    "output = poem_chain.invoke({'school': 'primary', 'topics': 'solar system', 'sentences': 2})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479bf9a5",
   "metadata": {},
   "source": [
    "What happens here?\n",
    "\n",
    "- The same input dictionary is sent to both fact_chain and poem_chain.\n",
    "- Both chains execute independently (in parallel in logic, not necessarily true multi-threaded parallelism).\n",
    "- Output is a dictionary:\n",
    "\n",
    "{'fact': <fact_output>, 'poem': <poem_output>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9842e0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the sun, eight planets, moons, asteroids, comets, and other celestial objects.\n",
      "2. The planets in the solar system orbit around the sun in elliptical paths.\n",
      "\n",
      "\n",
      "\n",
      "Planets dance in space,\n",
      "Sun's light gives them grace.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "chain = RunnableParallel(fact = fact_chain, poem = poem_chain)\n",
    "\n",
    "\n",
    "output = chain.invoke({'school': 'primary', 'topics': 'solar system', 'points': 2, 'sentences': 2})\n",
    "print(output['fact'])\n",
    "print('\\n\\n')\n",
    "print(output['poem'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0384b",
   "metadata": {},
   "source": [
    "### Chain Router\n",
    "- The router chain is used to route the output of a previous runnable to the next runnable based on the output of the previous runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c143a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Given the user review below, classify it as either being about `Positive` or `Negative`.\n",
    "            Do not respond with more than one word.\n",
    "\n",
    "            Review: {review}\n",
    "            Classification:\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "review = \"Thank you so much for providing such a great plateform for learning. I am really happy with the service.\"\n",
    "# review = \"I am not happy with the service. It is not good.\"\n",
    "chain.invoke({'review': review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a960ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prompt = \"\"\"\n",
    "                You are expert in writing reply for positive reviews.\n",
    "                You need to encourage the user to share their experience on social media.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "positive_template = ChatPromptTemplate.from_template(positive_prompt)\n",
    "positive_chain = positive_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f49fc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"\"\"\n",
    "                You are expert in writing reply for negative reviews.\n",
    "                You need first to apologize for the inconvenience caused to the user.\n",
    "                You need to encourage the user to share their concern on following Email:'udemy@kgptalkie.com'.\n",
    "                Review: {review}\n",
    "                Answer:\"\"\"\n",
    "\n",
    "\n",
    "negative_template = ChatPromptTemplate.from_template(negative_prompt)\n",
    "negative_chain = negative_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0b76b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rout(info):\n",
    "    if 'positive' in info['sentiment'].lower():\n",
    "        return positive_chain\n",
    "    else:\n",
    "        return negative_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16030065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  sentiment: ChatPromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['review'], input_types={}, partial_variables={}, template='Given the user review below, classify it as either being about `Positive` or `Negative`.\\n            Do not respond with more than one word.\\n\\n            Review: {review}\\n            Classification:'), additional_kwargs={})])\n",
       "             | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x1183d38d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11839d250>, root_client=<openai.OpenAI object at 0x1185601d0>, root_async_client=<openai.AsyncOpenAI object at 0x11925c550>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "             | StrOutputParser(),\n",
       "  review: RunnableLambda(lambda x: x['review'])\n",
       "}\n",
       "| RunnableLambda(rout)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"sentiment\": chain, 'review': lambda x: x['review']} | RunnableLambda(rout)\n",
    "\n",
    "full_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db480de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear valued customer,\n",
      "\n",
      "We are truly sorry to hear that you are not happy with the service provided. Your satisfaction is our top priority and we apologize for any inconvenience caused.\n",
      "\n",
      "We would like to hear more about your experience and address your concerns in a timely manner. Please feel free to share your feedback with us via email at udemy@kgptalkie.com. Your input is highly valuable to us and we are committed to improving our services based on your feedback.\n",
      "\n",
      "Thank you for bringing this to our attention and giving us the opportunity to make things right.\n",
      "\n",
      "Best regards,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "# review = \"Thank you so much for providing such a great platform for learning. I am really happy with the service.\"\n",
    "review = \"I am not happy with the service. It is not good.\"\n",
    "\n",
    "output = full_chain.invoke({'review': review})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f99acd",
   "metadata": {},
   "source": [
    "### Make Custom Chain Runnables with RunnablePassthrough and RunnableLambda\n",
    "- This is useful for formatting or when you need functionality not provided by other LangChain components, and custom functions used as Runnables are called RunnableLambdas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3bf1364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input1', 'input2'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input1', 'input2'], input_types={}, partial_variables={}, template='Explain these inputs in 5 sentences: {input1} and {input2}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "def char_counts(text):\n",
    "    return len(text)\n",
    "\n",
    "def word_counts(text):\n",
    "    return len(text.split())\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Explain these inputs in 5 sentences: {input1} and {input2}\")\n",
    "\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8594e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The input \"Earth is planet\" is stating that Earth is a celestial body that orbits around the Sun, similar to other planets in our solar system.\n",
      "2. The input \"Sun is star\" is indicating that the Sun is a massive, luminous sphere of plasma that emits light and heat, just like other stars in the universe.\n",
      "3. Both inputs are basic astronomical facts that are commonly taught in science classes and are fundamental to understanding the structure and dynamics of our solar system.\n",
      "4. The relationship between Earth and the Sun is crucial for sustaining life on our planet, as the Sun provides the energy necessary for photosynthesis and other essential processes.\n",
      "5. Understanding the roles of Earth as a planet and the Sun as a star helps us appreciate the vastness and complexity of the universe and our place within it.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "output = chain.invoke({'input1': 'Earth is planet', 'input2': 'Sun is star'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10aaae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'char_counts': 701, 'word_counts': 122, 'output': '1. The input \"Earth is planet\" is stating that Earth is a celestial body that orbits around the Sun, similar to other planets in our solar system.\\n2. The input \"Sun is star\" is indicating that the Sun is a massive, luminous sphere of plasma held together by its own gravity, much like the billions of other stars in the universe.\\n3. Both inputs are basic astronomical facts that are widely accepted by scientists and astronomers.\\n4. The relationship between Earth and the Sun is that Earth revolves around the Sun, which provides the heat and light necessary for life on our planet.\\n5. Understanding these inputs helps us comprehend the vastness and complexity of the universe and our place within it.'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | StrOutputParser() | {'char_counts': RunnableLambda(char_counts), \n",
    "                                            'word_counts': RunnableLambda(word_counts), \n",
    "                                            'output': RunnablePassthrough()}\n",
    "\n",
    "output = chain.invoke({'input1': 'Earth is planet', 'input2': 'Sun is star'})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645bf4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3393ef6d",
   "metadata": {},
   "source": [
    "### Custom Chain using `@chain` decorator\n",
    "\n",
    "Custom Chains with @chain Decorator in LangChain (LCEL, v0.3.x)\n",
    "\n",
    "1. What is the `@chain` Decorator?\n",
    "\n",
    "- The @chain decorator is a feature of LangChain’s LCEL API (Expression Language).\n",
    "- It transforms any standard Python function into a Runnable Chain.\n",
    "- This allows you to wrap arbitrary Python logic—including loops, conditionals, and composition of other chains—into a component that behaves just like any other chain in LCEL (i.e., it supports .invoke(), .batch(), etc.).\n",
    "\n",
    "2. Why Use `@chain`?\n",
    "\n",
    "For complex, custom workflows that can’t be easily expressed by chaining with the | operator.\n",
    "To combine multiple sub-chains, add custom logic, or perform steps that need Python code (like aggregation, formatting, conditional logic).\n",
    "It enables full flexibility within the LCEL framework: your custom function becomes a first-class Runnable.\n",
    "\n",
    "3. Example Explained\n",
    "\n",
    "\n",
    "4. How Does it Work?\n",
    "\n",
    "The decorator wraps your function into a Runnable-compatible object.\n",
    "Supports all LCEL operations (invoke, batch, etc.).\n",
    "You can include arbitrary Python code—loops, conditionals, error handling, etc.\n",
    "The function input is a single parameter (often a dict).\n",
    "The output can be any object—dict, list, string, etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf85db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def custom_chain(params):\n",
    "    return {\n",
    "        'fact': fact_chain.invoke(params),\n",
    "        'poem': poem_chain.invoke(params),\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a7580",
   "metadata": {},
   "source": [
    "\n",
    "4. How Does it Work?\n",
    "\n",
    "- The decorator wraps your function into a Runnable-compatible object.\n",
    "- Supports all LCEL operations (invoke, batch, etc.).\n",
    "- You can include arbitrary Python code—loops, conditionals, error handling, etc.\n",
    "- The function input is a single parameter (often a dict).\n",
    "- The output can be any object—dict, list, string, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "055a1805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The solar system consists of the sun, eight planets, moons, asteroids, comets, and other celestial objects.\n",
      "2. The planets in the solar system orbit around the sun in elliptical paths.\n",
      "\n",
      "\n",
      "\n",
      "Planets dance in space,\n",
      "Sun's light gives them grace.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {'school': 'primary', 'topics': 'solar system', 'points': 2, 'sentences': 2}\n",
    "output = custom_chain.invoke(params)\n",
    "print(output['fact'])\n",
    "print('\\n\\n')\n",
    "print(output['poem'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
